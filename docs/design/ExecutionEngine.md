# **Image Processing Pipeline: Execution Engine Architecture**

## **Table of Contents**

1. [Architectural Style](#1-architectural-style)
2. [Structural Components](#2-structural-components)
3. [Interaction Patterns](#3-interaction-patterns)
4. [Execution Lifecycle](#4-execution-lifecycle)
5. [Resource Management](#5-resource-management)
6. [Optimization Strategy: Runtime Adaptation](#6-optimization-strategy-runtime-adaptation)
7. [Fault Tolerance & Error Handling](#7-fault-tolerance--error-handling)
8. [Implementation Notes](#8-implementation-notes)
9. [References](#9-references)
10. [Appendix A: Future Enhancements & Known Limitations](#a-appendix-future-enhancements--known-limitations)
11. [Appendix B: Diagrams](#b-appendix-diagrams)

## **1\. Architectural Style**

The Execution Engine implements a **Pipes and Filters** architecture governed by a **Dataflow** model.

* **Filters (Blocks):** Stateless processing units responsible for transforming data.
* **Pipes (Links):** Managed by "Warehouses" (Data) and "Barriers" (Control) that handle buffering and synchronization.
* **Control Flow:** Driven by data availability (Data-Driven), orchestrated by a central runtime (The Engine).
* **Concurrency Model:** Parallel block execution on .NET ThreadPool.

## **2\. Structural Components**

### **2.1. The Warehouse (Output Buffer)**

The "Warehouse" is a storage component attached to the **Output Face** of a Producer Block.

* **Affinity:** **Producer-Centric (Upstream).** It collects all results generated by the block.
* **Responsibility:**
    1. **Storage:** Holds the `IDictionary<Socket, IReadOnlyList<WorkItem>>` produced by the block (immutable after commit).
    2. **Inventory Tracking:** Maintains a **Consumer Counter** (`int32` atomically decremented) initialized to the Output Socket's **Out-Degree** (number of downstream links).
    3. **Distribution:** Serves data to consumers upon request, implementing JIT Cloning logic (see Section 3.2).
* **Thread Safety:**
    - Counter updates use `Interlocked.Decrement`.
    - Data reads for *cloning* (intermediate consumers) are lock-free.
    - Data import and final cleanup (last consumer) use a fine-grained `Lock` to ensure consistency.
* **Lazy Initialization:** Warehouses are allocated on-demand when a block first produces output.

### **2.2. The Dependency Barrier (Control Gate)**

The "Barrier" is a lightweight control structure attached to the **Consumer Block**.

* **Affinity:** **Consumer-Centric (Downstream).**
* **Responsibility:**
    1. **Readiness Tracking:** Maintains a **Dependency Counter** (`int32` atomically decremented) initialized to the Block's **In-Degree** (Total incoming connections).
    2. **Signaling:** When the counter reaches zero via `Interlocked.Decrement`, it atomically enqueues the Block to the Engine's **Ready Queue** (once per cycle).
* **Implementation:** Uses a simple `int32` counter + atomic `int32` flag:

    ```csharp
    private int dependencyCounter;      // Initialized to In-Degree
    private int enqueuedFlag = 0;       // 0 = not enqueued, 1 = enqueued

    public bool TrySignalReady() {
        int remaining = Interlocked.Decrement(ref dependencyCounter);
        if (remaining == 0) {
            // Atomically set flag to prevent duplicate enqueue
            if (Interlocked.CompareExchange(ref enqueuedFlag, 1, 0) == 0) {
                Engine.EnqueueReady(this.BlockId);
                return true;
            }
        }
        return false;
    }
    ```

    **Rationale:** Avoids `CountdownEvent` overhead. The `CompareExchange` ensures exactly one enqueueing.

> **Note:** Barriers are not recycled between cycles. They will be re-initialized.

* **Lazy Initialization:** Barriers are allocated only when a block's first predecessor completes.

### **2.3. The Engine (Orchestrator)**

The Engine functions as a **Process Manager**. It is responsible for:

1. **Lifecycle Management:** Instantiating blocks, warehouses, and barriers.
2. **Topology Verification:** Static analysis of the DAG (Tarjan's algorithm for cycle detection).
3. **Task Dispatching:** Dispatching "Ready" blocks based on "backpressure"-guided DFS using a priority queue (`PriorityQueue<IBlock, Priority>`).
4. **Error Aggregation:** Capturing exceptions, marking downstream blocks as "blocked"/"failed", and aggregating failures.

### **2.4. The Scheduler**

The Scheduler is responsible for determining the order of execution. It abstracts the policy (e.g., Simple DFS vs. Adaptive) from the mechanism.

* **Interface (`IScheduler`):**
    - **Readiness Tracking:** Manages the "Ready Queue" of blocks whose dependencies are satisfied.
    - **Prioritization:** Decides which ready block to execute next based on the active strategy (e.g., Greedy Completion Pressure).
    - **Completion Handling:** Signals downstream barriers upon block completion.
    - **Cycle Management:** Handles shipment cycle transitions for batched execution.
* **Affinity:** **Engine-Centric.** It is a component of the Engine.

## **3\. Interaction Patterns**

### **3.1. Synchronization Protocol**

Synchronization is split between Data Availability (Producer) and Dependency Resolution (Consumer).

1. **Production:** Block executes and places results in its **Warehouse** (atomic commit).
2. **Notification:** The Scheduler identifies all connected downstream Barriers and atomically decrements their **Dependency Counters** using `Interlocked.Decrement`.
3. **Activation:** If a Barrier reaches zero, the Consumer Block is enqueued to the **Ready Queue**.
4. **Deadlock Prevention:** The Engine maintains a watchdog timer; if no progress occurs within a configurable timeout (default: 30s), it performs liveness analysis (not implemented) and throws `PipelineDeadlockException`.

### **3.2. JIT Cloning & Reference Handover**

To optimize memory usage, cloning is deferred until the exact moment of dispatch (Pull-based).

* **Logic:** When a Consumer Block is dispatched, it requests inputs from the upstream Warehouses.
* **Check:** The Warehouse uses `Interlocked.Decrement` to atomically decrement its **Consumer Counter**.
    * **Case A (Counter after decrement ≥ 1):** Other consumers are still waiting. The Warehouse creates a **Defensive Clone** `IBasicWorkItem`'s `Clone` contract and returns the clone.
    * **Case B (Counter after decrement == 0):** This is the last consumer. The Warehouse transfers the **Original Reference** (ownership transfer). The internal buffer is cleared.
* **Constraint:** `WorkItem` wraps `Image<TPixel>` from SixLabors.ImageSharp. When cloning, use `image.Clone()`. When transferring (Case B), the original `WorkItem` is moved.
* **Disposal Semantics:**
    - After a block finishes execution, the Engine **must** call `workItem.Dispose()` on all consumed inputs (Section 5.2). This disposes the underlying `Image<TPixel>`.
    - **Warehouse Cleanup:** Once Counter reaches 0, the Warehouse clears its internal reference (no explicit disposal—GC handles it if no transfer occurred).

* **Race Condition Prevention Reference:**
    ```csharp
    int remaining = Interlocked.Decrement(ref consumerCounter);
    if (remaining == 0) {
        return MoveOriginal(); // Last consumer
    } else {
        return CreateClone();  // Still has consumers
    }
    ```

## **4\. Execution Lifecycle**

### **Phase 1: Static Validation**

1. **Sink Verification:** Confirms at least one Sink/Save Block exists.
2. **Source Verification:** Confirms at least one Source/Load Block exists.
3. **Port Binding:** Ensures all mandatory input ports are bound.
4. **Cycle Detection:** Verifies the graph is a Directed Acyclic Graph (DAG).
5. ~~**Type Checking:** Validates data contracts.~~ (Deferred)

### **Phase 2: Initialization**

* **State Construction:**
    - **Barriers:** Created lazily via `ConcurrentDictionary<BlockId, Lazy<Barrier>>`.
    - **Warehouses:** Created on-demand when blocks first produce output.
* **Counter Setup:**
    * Warehouse Counters = Out-Degree (Fan-Out) - initialized when warehouse is first created.
    * Barrier Counters = In-Degree (Fan-In) - initialized when barrier is first accessed.
* **Ready Queue Bootstrap:** All source blocks are enqueued immediately.
* **Shipment Source Initialization:** Blocks implementing `IShipmentSource` (typically LoadBlock) have their `MaxShipmentSize` property configured from `ExecutorConfiguration.MaxShipmentSize` (default: 64).

### **Phase 2.5: Shipment-Based Execution**

**Problem:** Loading all images at once (e.g., 10,000 WorkItems) causes memory explosion and poor watchdog behavior (single progress tick for entire load operation).

**Solution:** Graph executes multiple times, and sources producing batches of work items.

#### **IShipmentSource Contract**

Sources should implement this interface to engage in batched execution.

```csharp
public interface IShipmentSource : IBlock
{
    int MaxShipmentSize { get; set; }
}
```

**Execution Flow (using standard Load block):**

1. **Bootstrap Phase:** `LoadBlock` stores file paths (not images) and initializes internal cursor/offset to 0.
2. **First Execution:** `LoadBlock.Execute()` loads up to `MaxShipmentSize` images (e.g., 64).
3. **Re-enqueue Decision:** Executor checks output count:
    - If `outputCount >= MaxShipmentSize`: `LoadBlock` remains active and is re-enqueued for the next cycle.
    - If `outputCount < MaxShipmentSize`: Mark LoadBlock as `Completed` (exhausted).
4. **Subsequent Executions:** The cycle repeats until all sources are exhausted.
5. **Pipeline Draining:** Downstream blocks (Resize, Save, etc.) process each shipment normally.

**Benefits:**

* **Memory Control:** Pipeline holds max 64 × NumBlocks WorkItems, not 10,000 × NumBlocks.
* **Watchdog-Friendly:** Processing 10,000 images = 157 shipments @ 64 each = 157 progress ticks (vs. 1 tick for "Load completed").

**Implementation Notes:**

* **Transparency:** Downstream blocks are unaware of shipments. They simply process available inputs.
* **State Persistence:** `LoadBlock` maintains internal state (cursor, file list) across executions (exceptional circumstances).
* **Exhaustion Detection:** `outputCount < MaxShipmentSize` signals no more data next cycle.
* **Progress Tracking:** Changed from `CompletedBlockCount` to `ProcessedShipmentCount` to reflect progress.

**Contrast with BatchWorkItem:**

* **Shipments:** Mechanism for memory-controlled processing (multiple executions).
* **BatchWorkItem:** Data structure for grouping images that must undergo identical processing (single execution, multiple images in one WorkItem).

### **Phase 3: Runtime Loop**

The Engine runs an asynchronous loop that orchestrates execution:

1. **Watchdog:** Monitors progress to detect deadlocks (see Section 3.1).
2. **Dispatch:** While concurrency limits allow:
    * **Dequeue:** Asks the Scheduler for the next "Ready" block.
    * **Execute:** Spawns a `Task` on the ThreadPool to execute the block.
3. **Block Execution:**
    * **Fetch:** Pulls data from upstream Warehouses (triggering JIT Cloning/Moving).
    * **Run:** Executes the block logic.
    * **Commit:** Stores results in the block's Warehouse.
    * **Notify:** Informs the Scheduler of completion (which signals downstream barriers).
4. **Cycle Management:** When all blocks complete, if sources remain active, resets state and begins the next shipment cycle.

## **5\. Resource Management**

### **5.1. Memory Efficiency (Greedy Completion Pressure)**

Since Warehouses hold data until *all* consumers have read it, "partial consumption" leads to memory waste. The Scheduler picks the block that clears the most expensive active buffers.

* **Formula:**

    $$Priority(B) = -\sum_{P \in Predecessors(B)} \frac{WarehouseSize(P)}{RemainingConsumers(P)}$$

    (Negative because PriorityQueue is min-heap)

* **Mechanism (Fan-Out Dilution):**
    * **Shared Parents (Low Priority):** If Block A feeds B and C (Out-Degree=2), the pressure to execute B is only $Size(A) / 2$. Executing B doesn't fully release A, so it's "cheaper" to ignore.
    * **Exclusive Parents (High Priority):** Once B executes and produces data for SaveB (Out-Degree=1), the pressure to execute SaveB is $Size(B) / 1$.
    * **Result:** Since $Size(B)/1 > Size(A)/2$, the scheduler prioritizes the exclusive child (SaveB) over the sibling (C).

* **Natural DFS:** The formula implicitly favors following a linear chain (where Out-Degree=1) over widening the graph (where Out-Degree > 1). This keeps the "working set" of active buffers deep and narrow, rather than shallow and wide.

* **Implementation:** Priority calculated locally at enqueue time (O(In-Degree)).

### **5.2. Deterministic Disposal**

* **Warehouse Cleanup:** Occurs automatically when the last consumer reads the data (Counter reaches 0). Internal reference is nulled (GC eligible).
* **Input Disposal:** Once a consumer block finishes execution, the Engine immediately calls `workItem.Dispose()` on all input WorkItems. This disposes the underlying `Image<TPixel>` objects.
* **Blocked Blocks:** If a block is marked "Blocked" (downstream of a failure), the Engine:
    1. Atomically decrements upstream Warehouse counters for that block's inputs (as if it consumed them). This ensures Warehouses are properly cleaned up even when consumers are skipped.
    2. Does NOT execute the block (no actual data consumption).
    3. The Engine scans for blocked downstreams as the consequence of this block's blocking and mark them too.

### **5.3. Global Memory Budget** *(Both Modes)*

To prevent OOM, the Engine enforces memory limits:

* **Tracking:**
    ```
    TotalMemoryUsage = Σ (WarehouseSize × WorkItem.SizeMP × BytesPerPixel)
    ```
    Updated atomically after each Warehouse commit using `Interlocked.Add`.

* **Soft Limit (Default: 50% of available RAM, configurable):**
    - When exceeded, Source blocks check `CanProduce()` before execution
    - If over limit: block sleeps 100ms, retries (cooperative backpressure)
    - Existing in-flight blocks continue (gradual drain)
    - Priority boost for blocks downstream of full Warehouses (×2.0)

* **Hard Limit (Default: 75% of available RAM, configurable):**
    - When exceeded, Engine has two modes:
        1. **Fail-Fast (Default):** Throw `OutOfMemoryException` with diagnostic data
        2. **Force-Drain (Optional):** Discard oldest Warehouse contents, mark consumers as failed

* **Memory Estimation:**
    - `BytesPerPixel = 4` for `Rgba32` (default)
    - Configurable per pixel format (`Rgb24 = 3`, `La16 = 2`, etc.)
    - Includes 10% overhead for metadata/internal structures

* **Startup Validation:**
    - If available RAM < 2× largest expected WorkItem, log warning
    - If available RAM < Hard Limit, fail initialization with clear error

## **6\. Optimization Strategy: Runtime Adaptation**

The Engine supports **two execution modes** (configurable via `ExecutionMode` enum):

### **Mode A: Greedy Completion Pressure (Default, Production-Ready)**

Uses pure **Greedy Completion Pressure** (see Section 5.1 for formula) with no live profiling or depth calculation. **Recommended for initial implementation and most workloads.**

* **Key Properties:**
    - **Simple:** No depth computation, no arbitrary constants (like ×1000 multipliers)
    - **Greedy:** Always picks the block that frees the most memory right now
    - **Natural DFS:** Paths with accumulated warehouses become progressively "hungrier"
    - **Overhead:** O(In-Degree) per enqueue—negligible for typical graphs
    - **Deterministic:** Priority calculated once at enqueue, no runtime updates

* **No Critical Path:** Avoids Bellman-Ford entirely.
* **No Profiling:** Skips cost tracking (Section 6.2/6.3 disabled).
* **Implementation Scope:** Core components only (Sections 1-5, 7-8). Skip Mode B entirely for MVP.

### **Mode B: Adaptive (Experimental, High-Complexity Workloads)**

⚠️ **DEFER UNTIL POST-MVP.** Extends Mode A with live profiling and critical path analysis. Use when:
- Pipelines have >20 blocks
- Block costs vary by >5× (e.g., heavy denoise vs. simple crop)
- Willing to accept ~2-5% scheduling overhead for 10-20% throughput gain

**Key Difference from Mode A:** Mode B **extends** Mode A's Greedy Completion Pressure with cost-aware adjustments.

#### **Shipment Boundary Micro-Optimization**

**Challenge:** With shipment-based execution, the pipeline completes a "mini-cycle" after each shipment (e.g., every 64 images). Mode B recomputes critical paths at these boundaries.

**Options:**

1. **Conservative (Recommended):** Only recompute after full cycles (LoadBlock exhausted).
    - **Pro:** Stable scheduling, avoids thrashing.
    - **Con:** Misses intra-shipment optimization opportunities.

2. **Aggressive:** Recompute after every shipment.
    - **Pro:** Adapts quickly to changing block costs.
    - **Con:** High overhead (157 recomputations for 10,000 images), scheduling instability.

3. **Hybrid (User-Configurable):** Add `ExecutorConfiguration.RecomputeAfterShipment` toggle.
    - **Default:** `false` (conservative)
    - **Advanced Users:** `true` for experimentation

**Implementation Guidance:** Start with Option 1 (conservative). Add Option 3 if profiling shows benefit for specific workloads.

---

### **6.1. Dynamic Priority Adjustment** *(Mode B Only)*

Mode B **extends** Mode A's Greedy Completion Pressure with runtime cost profiling:

* **Base Priority:** Same as Mode A (Section 5.1):

    $$Priority_{base}(B) = -\sum_{P \in Predecessors(B)} \frac{WarehouseSize(P)}{RemainingConsumers(P)}$$

* **Cost-Weighted Boost (Dynamic):** Applied when dequeueing from Ready Queue:

    $$Priority_{runtime}(B) = Priority_{base}(B) - \alpha \times \hat{Cost}(B) \times AvgInputSize(B)$$

    (Where $Priority_{base}(B)$ is defined in Section 5.1)

    where:
    - $\hat{Cost}(B)$ = profiled cost per megapixel (Section 6.2)
    - $AvgInputSize(B)$ = average input WorkItem size in megapixels
    - $\alpha = 0.1$ (tunable, balances memory pressure vs. throughput)
    - **Sign Convention:** Negative because expensive blocks should run sooner to prevent downstream starvation. More negative = higher priority (min-heap).

* **Critical Path Multiplier:** If B is on the current critical path (Section 6.3):

    $$Priority_{runtime}(B) = Priority_{runtime}(B) \times 1.5$$

* **Update Frequency:** Priorities recalculated at dequeue time to reflect current Warehouse states and profiled costs (O(In-Degree + 1) cost).

* **Fallback Behavior:** If profiling data is insufficient (< 5 samples for block type), Mode B falls back to pure Mode A priority (cost term omitted).

> **Rationale:** Mode A's greedy approach provides natural DFS behavior through memory pressure alone. Mode B adds cost-awareness to prioritize expensive blocks earlier, preventing late-stage bottlenecks in pipelines with heterogeneous block costs. The multiplicative critical path boost ensures throughput-critical blocks aren't starved by memory-hungry but cheap blocks.

### **6.2. Incremental Cost Profiling** *(Mode B Only)*

Since execution is non-deterministic, the Engine maintains a **Rolling Statistics Window** per block type.

* **Metric Collection:** After each block execution, record:

    $$Sample_i = \left( T_{exec,i}, PixelCount_i, \frac{T_{exec,i}}{PixelCount_i} \right)$$

    where $PixelCount_i$ is stored in `WorkItem.SizeMP` (megapixels, precomputed: `Width × Height / 1,000,000`)

* **Cost Estimate (Exponential Moving Average):**

    $$\hat{Cost}(B) = \alpha \times \frac{T_{current}}{PixelCount_{current}} + (1-\alpha) \times \hat{Cost}_{previous}(B)$$

    where $\alpha = 0.2$ (emphasizes recent behavior, adapts to thermal throttling).

* **Variance Tracking:** Maintains $\sigma^2$ to detect unstable blocks (high variance triggers conservative scheduling).

* **Persistence:** Statistics serialized to `~/.cache/pipeline/profile.db` (SQLite) and restored on next startup.

### **6.3. Critical Path Identification (Live)** *(Mode B Only)*

The critical path is recomputed **conditionally** to avoid excessive overhead.

> **Note:** Critical path analysis is exclusive to Mode B. Mode A relies solely on Greedy Completion Pressure without path-based adjustments.

#### **Simple Heuristic (Default):**

Recompute when ANY of:
1. **Performance Deviation:** Live execution time deviates >20% (configurable) from $\hat{Cost}(B) \times WorkItem.SizeMP$ for blocks on the *previous* critical path.
2. **Sampling Interval:** Every 10 blocks (configurable) complete.
3. **Time-Based:** Every 5 seconds (configurable) of wall-clock time.

**Algorithm:** Modified Bellman-Ford with negative edge weights:

$$Weight(A \to B) = -\hat{Cost}(A) \times AvgWorkItemSize(A)$$

Longest path from sources to sinks = critical path. Complexity: O(V + E) per recomputation.

**Priority Boost:** Blocks on the critical path receive multiplicative boost (×1.5) applied to their runtime priority (see Section 6.1).

#### **Advanced Strategy (Optional, Experimental):**

**Batch Scheduling with Grouped Recomputation:**

* **Mechanism:** Instead of dispatching blocks immediately when ready, accumulate a "batch" of K ready blocks (K=5 default, configurable).
* **Dispatch:** Sort batch by priority, dispatch all K in parallel.
* **Recomputation:** Recalculate critical path **once** after all K complete (amortizes Bellman-Ford cost).
* **Trade-off:**
    - ✅ Reduces scheduling overhead by ~80% (1 recomputation per K blocks instead of K).
    - ❌ Introduces latency: blocks wait for batch to fill.
    - ❌ Less responsive to sudden cost changes.
* **Best For:** High-throughput batch processing (e.g., 1000+ images).

**Implementation Note:** Enabled via `ExecutionMode.AdaptiveBatched` (distinct from `ExecutionMode.Adaptive`).

### **6.4. Calibration Mode (First-Run Only)** *(Mode B Only)*

On first startup (no cached profiles), the Engine runs a **Synthetic Benchmark Suite**:

* **Test Set:** 10 synthetic images per block type:
    - 3× 1080p grayscale (1.9 MP)
    - 3× 4K RGB (8.3 MP)
    - 2× 4K RGBA (8.3 MP)
    - 2× 8K grayscale (33.2 MP)

* **Output:** Seeds initial $\hat{Cost}(B)$ estimates with 95% confidence intervals.

* **Fallback:** If calibration fails (e.g., insufficient memory), uses conservative defaults based on block category:
    - **Pixel Operations:** 0.5 ms/MP
    - **Convolution Filters:** 5.0 ms/MP
    - **Transforms (FFT, etc.):** 15.0 ms/MP
    - **I/O Operations:** 50.0 ms/MP

### **6.5. Thermal & Load Adaptation** *(Both Modes)*

The Engine monitors system conditions and adjusts parallelism:

* **CPU Throttling Detection:** If block durations increase >30% across 5 consecutive executions, reduce `MaxDegreeOfParallelism` by 25%.

* **Memory Pressure:** If GC collections exceed 10/second, pause new block dispatches until pressure subsides (prevents thrashing).

* **External Load:** Uses `Environment.ProcessorCount` and current CPU usage (via `PerformanceCounter`) to avoid oversubscription.

## **7\. Fault Tolerance & Error Handling**

### **7.1. Exception Propagation**

* **Block Failure:** If a block throws an exception, the Engine:
    1. Marks all transitive downstream blocks as "Blocked" (skipped execution).
    2. **Warehouse Counter Cleanup:** For each blocked block, atomically decrements its upstream Warehouse counters (as if it consumed the data). This ensures Warehouses are released even when consumers don't run.
    3. Collects the exception into an `AggregateException`.
    4. Allows independent pipeline branches to continue (partial failure tolerance).
* **Fatal Errors:** Out-of-memory or stack overflow abort the entire pipeline immediately.

### **7.2. Cancellation Support**

* **Cooperative Cancellation:** Engine accepts `CancellationToken`. Blocks poll the token before heavy operations.
* **Cleanup:** On cancellation, the Engine disposes all Warehouses and marks the pipeline as "Aborted".

> **Note:** Block **MUST** implement support for cancellation token to accept cancellation request from Engine.

## **8\. Implementation Notes**

### **8.1. Data Structures**

* **Ready Queue:** `System.Collections.Generic.PriorityQueue<IBlock, float>` (guarded by lock).
* **Warehouse Storage:** `ImmutableList<WorkItem>` (guarded by lock for write/clear, lock-free for read).
* **Barrier Counter:** `int` with `Interlocked` operations (lock-free).
* **Lazy Components:** `ConcurrentDictionary<IBlock, Lazy<T>>` for on-demand initialization.

### **8.2. Threading Model**

* **Execution:**
    - **Default:** `Task.Run` (runs on ThreadPool).
    - **Long Running:** Not yet implemented (Mode B).
* **Coordination:** Single async loop in `GraphExecutor`.

### **8.3. Concurrency & Synchronization**

The system employs a **Hybrid Concurrency Model** combining atomic operations for high-frequency counters and fine-grained locks for structural state:

* **Atomic Operations (`Interlocked`):**
    - Used for **Dependency Barriers** (signaling readiness).
    - Used for **Warehouse Consumer Counts** (tracking remaining consumers).
    - Ensures minimal overhead for the most frequent operations (signaling and checking availability).

* **Fine-Grained Locks (`System.Threading.Lock`):**
    - **Scheduler Queue:** Protects the PriorityQueue. Since enqueue/dequeue frequency is lower than block execution time, contention is low.
    - **Warehouse Inventory:** Protects the import (commit) and the final cleanup (when count reaches zero).
    - **Active Sources:** Protects the list of active shipment sources.

* **Rationale:**
    - Pure lock-free structures (like `ConcurrentPriorityQueue`) add significant complexity and are often slower than uncontended locks in .NET.
    - The hybrid approach balances simplicity and performance, using atomics where contention is highest.

### **8.4. WorkItem Structure**

```csharp
public sealed class WorkItem : IDisposable
{
    public Image Image { get; }       // SixLabors.ImageSharp
    public float SizeMP { get; }      // Width × Height / 1,000,000
    public IImmutableDictionary<string, object> Metadata { get; }

    public WorkItem Clone() => new WorkItem(
        Image.Clone(x => {}),         // Deep copy of pixel data
        SizeMP,
        Metadata                      // Shallow copy (immutable)
    );

    public void Dispose() => Image.Dispose(); // Release unmanaged pixel buffer
}
```

## **9\. References**

1. **POSA:** Buschmann, F., et al. (1996). *Pattern-Oriented Software Architecture Volume 1*. (Pipes and Filters).
2. **EIP:** Hohpe, G., & Woolf, B. (2003). *Enterprise Integration Patterns*. (Message Store, Content-Based Router).
3. **GoF:** Gamma, E., et al. (1994). *Design Patterns*. (Observer, Prototype).
4. **Memory Management:** Jones, R., et al. (2011). *The Garbage Collection Handbook*. (Reference Counting).
5. **Concurrent Data Structures:** Herlihy, M., & Shavit, N. (2012). *The Art of Multiprocessor Programming*. (Lock-free algorithms).
6. **Graph Algorithms:** Cormen, T. H., et al. (2009). *Introduction to Algorithms, 3rd Ed*. (Topological sort, critical path).
7. **Adaptive Systems:** Hellerstein, J. L., et al. (2004). *Feedback Control of Computing Systems*. (Runtime adaptation, PID controllers).

---

## **A. Appendix: Future Enhancements & Known Limitations**

### **A.1. Scalability: Priority Queue Contention**

**Current Bottleneck:**
Under extreme parallelism (>100 concurrent threads), the single `ConcurrentPriorityQueue` can become a contention point even with lock-free operations. The head node experiences high CAS retry rates during concurrent dequeues.

**Proposed Solution: Two-Tier Scheduling**

Partition the ready queue into two tiers:

* **Tier 1 (Fast Path):** FIFO queue for blocks with uniform priority (Mode A) or low memory pressure. Uses work-stealing deques per thread to eliminate contention.
* **Tier 2 (Slow Path):** Priority queue for memory-pressure-sensitive blocks (high Completion Pressure score). Only accessed when Tier 1 is empty or memory exceeds thresholds.

**Trade-offs:**
- ✅ Reduces contention by 80-90% (measured in Go's runtime scheduler)
- ✅ Maintains priority semantics where needed (memory-critical blocks)
- ❌ Adds significant complexity: per-thread queues, work-stealing protocol, queue selection heuristics
- ❌ Requires ~50% more code in the scheduler

**Recommendation:** Implement only if profiling shows >10% time spent in queue operations under production workloads. Current design is sufficient for ≤100 threads.

---

### **A.2. Memory Management: Predictive Backpressure**

**Current Limitation:**
Section 6.5's GC-based throttling is **reactive** (waits for 10 GC/sec before pausing). The system can accumulate excessive in-flight memory before throttling triggers, leading to:
- Sudden pauses (all dispatches halt)
- Risk of OOM on memory-constrained systems
- No global cap on total Warehouse memory

**Critical Gap Example:**
```
10 Source blocks × 10 images/sec × 8K (32MB/image) = 3.2 GB/sec production rate
If downstream is 2× slower → 6.4 GB accumulation in 2 seconds
System with 8GB RAM → OOM before GC throttling activates
```

**Proposed Solution: Global Memory Budget with Soft/Hard Limits** (Section 5.3)

**Implementation Priority:** **CRITICAL**. This is a production blocker—must be added before release.

---

### **A.3. Clarification: Priority Staleness**

**Issue:** The term "lazy priority recalculation" could be misinterpreted as "priorities can be stale at dequeue time."

**Actual Behavior (Both Modes):**

Priorities are computed **on-demand during dequeue**, not cached:

* **Mode A:** Each `Dequeue()` calculates Completion Pressure from current Warehouse states.
* **Mode B:** Each `Dequeue()` calculates Completion Pressure + Cost-Weighted Boost + Critical Path Multiplier.

**Dequeue Process:**
1. Peek at top K candidates (K=3 default)
2. Recalculate `Priority(B)` for each using current Warehouse states (and profiled costs in Mode B)
3. Select highest priority (most negative value)
4. Return that block

**Cost:**
- Mode A: O(K × In-Degree) = ~6-9 operations per dequeue
- Mode B: O(K × (In-Degree + 1)) = ~9-12 operations per dequeue (adds cost lookup)

**No architectural change needed**—this is a documentation clarification.

---

### **A.4. Advanced Research: Adaptive Work Batching**

**Concept:** Instead of recalculating priorities per-block, accumulate batches of K ready blocks before scheduling.

**Mechanism:**
1. Wait until K blocks are ready (K=5 default)
2. Sort batch by priority once
3. Dispatch all K in parallel
4. Recalculate critical path once after all K complete

**Benefits:**
- Amortizes Bellman-Ford cost (O(V+E) every K blocks instead of every block)
- Reduces priority calculation overhead by ~80%

**Drawbacks:**
- Adds latency: blocks wait for batch to fill
- Less responsive to dynamic changes (e.g., sudden memory pressure)
- Requires careful tuning of K (too large = latency, too small = no benefit)

**Status:** Mentioned in Section 6.3 as `ExecutionMode.AdaptiveBatched`. Experimental—do not enable for production until validated with real workloads.

---

### **A.5. Known Limitations**

1. **No GPU Memory Tracking:** Current memory budget only tracks CPU RAM. GPU-accelerated blocks (if implemented) would need separate tracking.
2. **No Network I/O Backpressure:** If blocks fetch from network sources, the memory budget doesn't account for in-flight network buffers.
3. **Barrier Allocation:** Even with lazy initialization, Barriers are allocated per-block (not per-execution). A graph with 1000 blocks uses ~8KB for Barriers even if only 10 run concurrently.
4. **Priority Queue Scalability:** Single queue design limits scalability beyond ~100 threads (see A.1).
5. **No Priority Inheritance:** If a high-priority block depends on a low-priority block, the system doesn't propagate priority (can cause priority inversion under heavy load).

---

### **A.6. Observability and Telemetry**

**Current Limitation:** Debugging parallel graph execution via logs is difficult due to interleaved execution traces.

**Proposed Enhancement:** Integrate OpenTelemetry .NET instrumentation to produce distributed traces:
    - **Spans:** One per block execution, tagged with `BlockID`, `ThreadID`, and `ExecutionMode`
    - **Visualization:** Export traces to Jaeger/Zipkin to visualize critical paths and "waterfall" execution diagrams

## B. Appendix: Diagrams

### B.1. High-level diagram

```mermaid
flowchart TB
    subgraph Control_Plane [Control Plane: Orchestration & Signaling]
        direction TB
        Engine[("<b>The Engine</b><br/>(Process Manager)")]
        ReadyQ[("<b>Ready Queue</b><br/>(Priority Sorted)")]
        Barrier[("<b>Dependency Barrier</b><br/>(Atomic Counter)")]
    end

    subgraph Data_Plane [Data Plane: Storage & Flow]
        direction TB
        Producer[("<b>Producer Block</b><br/>(Filter)")]
        Warehouse[("<b>The Warehouse</b><br/>(Output Buffer & Inventory)")]
        Consumer[("<b>Consumer Block</b><br/>(Filter)")]
    end

    %% Execution Flow
    Producer -- "1. Executes & Commits Result" --> Warehouse
    Producer -- "2. Notify Completion" --> Engine

    %% Signaling Flow
    Engine -- "3. Decrement Counter" --> Barrier
    Barrier -- "4. Counter == 0? Signal Ready" --> ReadyQ
    ReadyQ -- "5. Dispatch (DFS Priority)" --> Engine
    Engine -- "6. Schedule on ThreadPool" --> Consumer

    %% Data Pull Flow
    Consumer -- "7. Pull Data (JIT Clone/Move)" --> Warehouse
    Warehouse -- "8. Return WorkItem" --> Consumer
```

### B.2. Detailed Diagram

```mermaid
flowchart LR
    subgraph Orchestrator ["The Engine (Orchestrator)"]
        direction TB
        Lifecycle[("<b>Lifecycle Manager</b><br/>Graph Validation & Instantiation")]
        Scheduler[("<b>Scheduler</b><br/>DFS Optimization Strategy")]
        ErrorHandler[("<b>Error Propagator</b><br/>Blocking & Cancellation")]
        Context[("<b>ExecutionContext</b><br/>Execution session internal states")]

        Lifecycle --> Scheduler
    end

    subgraph Execution ["Worker Environment"]
        Thread[".NET ThreadPool Worker"]
        WorkItem["<b>WorkItem</b><br/>Image&lt;TPixel&gt; + Metadata"]
    end

    subgraph Storage ["The Warehouse (Producer-Centric)"]
        direction TB
        Buffer[("<b>Immutable Buffer</b><br/>Dict&lt;Socket, List&lt;WorkItem&gt;&gt;")]
        ConsCounter[("<b>Consumer Counter</b><br/>Atomic Int32 (Out-Degree)")]
        JITLogic{"<b>JIT Logic</b><br/>(Interlocked.Decrement)"}

        Buffer
        JITLogic --> ConsCounter
        ConsCounter -- "Remaining > 0" --> Clone["<b>Defensive Clone</b><br/>(Deep Copy)"]
        ConsCounter -- "Remaining == 0" --> Move["<b>Reference Handover</b><br/>(Ownership Transfer)"]

    end

    subgraph Synchronization ["The Barrier (Consumer-Centric)"]
        direction TB
        DepCounter[("<b>Dependency Counter</b><br/>Atomic Int32 (In-Degree)")]
        EnqFlag[("<b>Enqueue Flag</b><br/>Atomic CAS (1 bit)")]

        DepCounter -- "Reach 0" --> EnqFlag
        EnqFlag -- "CompareExchange(0,1)" --> Signal["Signal Ready"]
    end

    %% Interactions
    Signal --> Scheduler
    Scheduler -- "Dequeue(Priority)" --> Thread
    Thread -- "Execute()" --> WorkItem
    Thread -- "Fetch Input" --> JITLogic
    Thread -- "Commit Output" --> Buffer

    %% Priority Logic Note
    Scheduler -.- PriorityNote["<b>Priority Formula:</b><br/>Sum(WarehouseSize / RemainingConsumers)"]
```

### B.3. Execution Diagram

```mermaid
%% { init: {"flowchart": {"defaultRenderer": "elk"}} }%%
sequenceDiagram
    autonumber
    participant Engine
    participant ThreadPool
    
    box Upstream
    participant BlockA as Block A (Source)
    participant WhA as Warehouse A
    end
    
    box Parallel Chain B
    participant BlockB as Block B
    participant WhB as Warehouse B
    end

    box Parallel Chain C
    participant BlockC as Block C
    participant WhC as Warehouse C
    end
    
    box Downstream
    participant BarrD as Barrier D
    participant BlockD as Block D (Sink)
    end

    note over Engine: PHASE 1: INITIALIZATION
    Engine->>ThreadPool: Schedule Source (A)
    activate BlockA
    BlockA->>WhA: Commit Result
    deactivate BlockA
    BlockA->>Engine: Notify Complete
    
    Engine->>Engine: Enqueue B & C
    
    note over Engine: PHASE 2: PARALLEL EXECUTION
    
    par Chain B
        Engine->>ThreadPool: Dispatch B
        activate BlockB
        
        note right of BlockB: 1. Gather
        BlockB->>WhA: Fetch Input (Clone)
        
        note right of BlockB: 2. Execute
        BlockB->>BlockB: Process
        
        note right of BlockB: 3. Push
        BlockB->>WhB: Commit Result
        deactivate BlockB
        
        note left of Engine: 4. Signal
        BlockB->>Engine: Notify Complete
        Engine->>BarrD: Signal (Decr 2->1)
    and Chain C
        Engine->>ThreadPool: Dispatch C
        activate BlockC
        
        note right of BlockC: 1. Gather
        BlockC->>WhA: Fetch Input (Move)
        note right of WhA: Ref Cleared
        
        note right of BlockC: 2. Execute
        BlockC->>BlockC: Process
        
        note right of BlockC: 3. Push
        BlockC->>WhC: Commit Result
        deactivate BlockC
        
        note right of Engine: 4. Signal
        BlockC->>Engine: Notify Complete
        Engine->>BarrD: Signal (Decr 1->0)
    end
    
    BarrD-->>Engine: Barrier Open (Ready)
    
    note over Engine: PHASE 3: MERGE
    Engine->>ThreadPool: Dispatch D
    activate BlockD
    BlockD->>WhB: Fetch Input
    BlockD->>WhC: Fetch Input
    BlockD->>BlockD: Execute
    deactivate BlockD
```

### B.4. Shipment Lifecycle Diagram

```mermaid
flowchart TD
    subgraph Cycle_Management [Shipment Cycle Loop]
        Start((Start)) --> InitSource["Init IShipmentSource<br/>(Cursor = 0)"]
        InitSource --> ExecSource[Execute Source Block]
        
        ExecSource --> CheckYield{Output Count >=<br/>MaxShipmentSize?}
        
        CheckYield -- Yes (More Data) --> MarkActive[Mark Source 'Active']
        MarkActive --> EnqueueDownstream[Signal Downstream Barriers]
        
        CheckYield -- No (Exhausted) --> MarkComp[Mark Source 'Completed']
        MarkComp --> EnqueueDownstream
        
        EnqueueDownstream --> Drain[<b>Drain Pipeline</b><br/>Execute all reachable blocks]
        
        Drain --> AllDone{All Blocks<br/>Finished?}
        AllDone -- No --> Drain
        AllDone -- Yes --> CheckSources{Any Source<br/>Active?}
        
        CheckSources -- Yes --> ResetGraph[Reset Barriers &<br/>Re-enqueue Sources]
        ResetGraph --> ExecSource
        
        CheckSources -- No --> Finish((End))
    end
```