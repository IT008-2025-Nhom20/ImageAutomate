# Image Processing Pipeline: Execution Engine Architecture

## Table of Contents

1. [Architectural Style](#1-architectural-style)
2. [Structural Components](#2-structural-components)
3. [Interaction Patterns](#3-interaction-patterns)
4. [Execution Lifecycle](#4-execution-lifecycle)
5. [Resource Management](#5-resource-management)
6. [Optimization Strategy: Runtime Adaptation](#6-optimization-strategy-runtime-adaptation)
7. [Fault Tolerance & Error Handling](#7-fault-tolerance--error-handling)
8. [Implementation Notes](#8-implementation-notes)
9. [References](#9-references)
10. [Appendix A: Future Enhancements & Known Limitations](#a-appendix-future-enhancements--known-limitations)
11. [Appendix B: Diagrams](#b-appendix-diagrams)
12. [Appendix C: Lexicon](#c-appendix-lexicon)

## 1. Architectural Style

The Execution Engine implements a **Pipes and Filters** architecture governed by a **Dataflow** model.

* **Filters:** `Blocks`[^C.1] — Stateless processing units responsible for transforming data.
* **Pipes:** `Links`[^C.3] — Managed by "`Warehouses`[^C.20]" (Data) and "`Barriers`[^C.21]" (Control) that handle buffering and synchronization.
* **Control Flow:** Driven by data availability (Data-Driven), orchestrated by a central runtime (the `Engine`[^C.22]).
* **Concurrency Model:** Parallel block execution on .NET ThreadPool.

## 2. Structural Components

### 2.1. The Warehouse (Output Buffer)

The `Warehouse`[^C.20] is a storage component attached to the output face of a producer `Block`[^C.1].

* **Affinity:** Producer-Centric (Upstream). It collects all results generated by the block.
* **Responsibility:**
    1. **Storage:** Holds the `IDictionary<Socket, IReadOnlyList<WorkItem>>` produced by the block (immutable after commit).
    2. **Inventory Tracking:** Maintains a **Consumer Counter** (`int32` atomically decremented) initialized to the output `Socket`'s `Fan-Out / Out-Degree`[^C.28] (number of downstream `links`[^C.3]).
    3. **Distribution:** Serves data to consumers upon request, implementing JIT Cloning logic (see Section 3.2).
* **Key Methods:**
    - `Import(IDictionary<Socket, IReadOnlyList<IBasicWorkItem>> outputs)`: Commits output data to the warehouse.
    - `GetInventory()`: Returns all data (always clones).
    - `GetInventory(IEnumerable<Socket> sockets)`: Returns data for specific sockets (always clones).
    - `DecrementConsumerCount()`: Decrements the consumer count for blocked block cleanup.
* **Properties:**
    - `RemainingConsumers`: Current count of remaining consumers.
    - `TotalSizeMp`: Total size in megapixels of stored items.
* **Thread Safety:**
    - Counter updates use `Interlocked.Decrement`.
    - Data reads for *cloning* (intermediate consumers) are lock-free.
    - Data import and final cleanup (last consumer) use a fine-grained `Lock` to ensure consistency.
* **Lazy Initialization:** Warehouses are allocated on-demand when a block first produces output.

### 2.2. The Dependency Barrier (Control Gate)

The `DependencyBarrier`[^C.21] is a lightweight control structure attached to the consumer block.

* **Affinity:** Consumer-Centric (Downstream).
* **Responsibility:**
    1. **Readiness Tracking:** Maintains a **Dependency Counter** (`int32` atomically decremented) initialized to the block's `Fan-In / In-Degree`[^C.27] (Total incoming connections).
    2. **Signaling:** When the counter reaches zero via `Signal()`, returns `true` to indicate the block should be enqueued.
* **Properties:**
    - `Block`: The `IBlock` this barrier is attached to.
    - `RemainingDependencies`: Current count of unsatisfied dependencies.
    - `HasEnqueued`: Whether the block has been enqueued (once per cycle).
* **Method:**
    ```csharp
    public bool Signal() {
        int remaining = Interlocked.Decrement(ref dependencyCounter);
        if (remaining == 0) {
            // Atomically set flag to prevent duplicate enqueue
            if (Interlocked.CompareExchange(ref enqueuedFlag, 1, 0) == 0) {
                return true;  // Block should be enqueued
            }
        }
        return false;
    }
    ```

    **Rationale:** Avoids `CountdownEvent` overhead. The `CompareExchange` ensures exactly one enqueueing.

> **Note:** Barriers are not recycled between cycles. They will be re-initialized.

* **Lazy Initialization:** Barriers are allocated only when a block's first predecessor completes.

### 2.3. The Engine (Orchestrator)

The `Engine`[^C.22] functions as a **Process Manager**. It is responsible for:

1. **Lifecycle Management:** Instantiating `blocks`[^C.1], `warehouses`[^C.20], and `barriers`[^C.21].
2. **Topology Verification:** Static analysis of the DAG (`Graph`[^C.5]) (Tarjan's algorithm for cycle detection).
3. **Task Dispatching:** Dispatching `Ready`[^C.7] `blocks` based on `Backpressure`[^C.30]-guided DFS using a priority queue (`PriorityQueue<IBlock, Priority>`).
4. **Error Aggregation:** Capturing exceptions, marking downstream `blocks` as `Blocked`[^C.9]/`Failed`[^C.10], and aggregating failures.

### 2.4. The Scheduler

The `Scheduler`[^C.23] is responsible for determining the order of execution. It abstracts the policy (e.g., Simple DFS vs. Adaptive) from the mechanism.

* **Interface (`IScheduler`):**
    - **Readiness Tracking:** Manages the "Ready Queue" of `blocks` whose dependencies are satisfied.
    - **Prioritization:** Decides which ready `block` to execute next based on the active strategy (e.g., Greedy `Completion Pressure`[^C.26]).
    - **Completion Handling:** Signals downstream `barriers` upon `block` `Completion`[^C.11].
    - **Cycle Management:** Handles `shipment`[^C.24] cycle transitions for batched execution.
* **Affinity:** Engine-Centric. It is a component of the `Engine`.

## 3. Interaction Patterns

### 3.1. Synchronization Protocol

Synchronization is split between Data Availability (Producer) and Dependency Resolution (Consumer).

1. **Production:** `Block` executes and places results in its `Warehouse` (atomic commit).
2. **Notification:** The `Scheduler` identifies all connected downstream `Barriers` and atomically decrements their `Dependency Counters` using `Interlocked.Decrement`.
3. **Activation:** If a `Barrier` reaches zero, the consumer `Block` is enqueued to the `Ready Queue`.
4. **Deadlock Prevention:** The `Engine` maintains a watchdog timer; if no progress occurs within a configurable timeout (default: 30s), it performs `Liveness`[^C.12] analysis (not implemented) and throws `PipelineDeadlockException` (preventing `Deadlock`[^C.31]).

### 3.2. JIT Cloning & Reference Handover

To optimize memory usage, cloning is deferred until the exact moment of dispatch (Pull-based).

* **Logic:** When a consumer `Block` is dispatched, it requests inputs from the upstream `Warehouses`.
* **Check:** The `Warehouse` uses `Interlocked.Decrement` to atomically decrement its `Consumer Counter`.
* **Defensive Cloning:** The `Warehouse` creates a `Clone` per `IBasicWorkItem`'s `Clone` contract and returns the clone. When the `warehouse` is empty (counter after decrement == 0), the `warehouse` nullifies its internal buffer for GC cleanup (ensuring `Safety`[^C.13]).
* **Constraint:** `WorkItem` wraps `Image<TPixel>` from SixLabors.ImageSharp. When cloning, use `image.Clone()`. When transferring (Case B), the original `WorkItem` is moved.
* **Disposal Semantics:**
    - After a `block` finishes execution, the `Engine` **must** call `workItem.Dispose()` on all consumed inputs (Section 5.2). This disposes the underlying `Image<TPixel>`.
    - **Warehouse Cleanup:** Once Counter reaches 0, the `Warehouse` clears its internal reference (no explicit disposal—GC handles it if no transfer occurred).

* **Warehouse Item Distribution Reference:**
    ```csharp
    int remaining = Interlocked.Decrement(ref consumerCounter);
    var clone = _internalBuffer.Clone();
    if (remaining == 0)
    {
        lock (bufferLock)
        {
            _internalBuffer = null; // Last consumer
        }
    }
    return clone;
    ```

## 4. Execution Lifecycle

### Phase 1: Static Validation

1. **Sink Verification:** Confirms at least one Sink/Save `Block` exists.
2. **Source Verification:** Confirms at least one Source/Load `Block` exists.
3. **Port Binding:** Ensures all mandatory input ports are bound.
4. **Cycle Detection:** Verifies the `Graph` is a directed acyclic `Graph`[^C.5] (DAG).
5. ~~**Type Checking:** Validates data contracts.~~ (Deferred)

### Phase 2: Initialization

* **State Construction:**
    - `Barriers`: Created lazily via `ConcurrentDictionary<BlockId, Lazy<Barrier>>`.
    - `Warehouses`: Created on-demand when `blocks` first produce output.
* **Counter Setup:**
    * `Warehouse` Counters = `Fan-Out / Out-Degree`[^C.28] - initialized when `warehouse` is first created.
    * `Barrier` Counters = `Fan-In / In-Degree`[^C.27] - initialized when `barrier` is first accessed.
* **Ready Queue Bootstrap:** All source `blocks` are enqueued immediately.
* **Shipment Source Initialization:** `Blocks` implementing `IShipmentSource` (typically LoadBlock) have their `MaxShipmentSize` property configured from `ExecutorConfiguration.MaxShipmentSize` (default: 64).

### Phase 2.5: Shipment-Based Execution

**Problem:** Loading all images at once (e.g., 10,000 `WorkItems`[^C.25]) causes memory explosion and poor watchdog behavior (single progress tick for entire load operation).

**Solution:** `Graph` executes multiple times, and sources producing batches of `work items`.

#### IShipmentSource Contract

Sources should implement this interface to engage in batched execution (`Shipment`[^C.24]).

```csharp
public interface IShipmentSource : IBlock
{
    int MaxShipmentSize { get; set; }
}
```

**Execution Flow** (using standard Load `Block`):

1. **Bootstrap Phase:** `LoadBlock` stores file paths (not images) and initializes internal cursor/offset to 0.
2. **First Execution:** `LoadBlock.Execute()` loads up to `MaxShipmentSize` images (e.g., 64).
3. **Re-enqueue Decision:** Executor checks output count:
    - If `outputCount >= MaxShipmentSize`: `LoadBlock` remains active and is re-enqueued for the next cycle.
    - If `outputCount < MaxShipmentSize`: Mark LoadBlock as `Completed`[^C.11] (exhausted).
4. **Subsequent Executions:** The cycle repeats until all sources are exhausted.
5. **Pipeline Draining:** Downstream `blocks` (Resize, Save, etc.) process each `shipment` normally.

**Benefits:**

* **Memory Control:** `Pipeline` holds max 64 × NumBlocks `WorkItems`, not 10,000 × NumBlocks.
* **Watchdog-Friendly:** Processing 10,000 images = 157 shipments @ 64 each = 157 progress ticks (vs. 1 tick for "Load completed").

**Implementation Notes:**

* **Transparency:** Downstream `blocks` are unaware of shipments. They simply process available inputs.
* **State Persistence:** `LoadBlock` maintains internal state (cursor, file list) across executions (exceptional circumstances).
* **Exhaustion Detection:** `outputCount < MaxShipmentSize` signals no more data next cycle.
* **Progress Tracking:** Changed from `CompletedBlockCount` to `ProcessedShipmentCount` to reflect progress.

**Contrast with BatchWorkItem:**

* **Shipments:** Mechanism for memory-controlled processing (multiple executions).
* **BatchWorkItem:** Data structure for grouping images that must undergo identical processing (single execution, multiple images in one `WorkItem`).

### Phase 3: Runtime Loop

The `Engine` runs an asynchronous loop that orchestrates execution:

1. **Watchdog:** Monitors progress to detect `deadlocks` (see Section 3.1).
2. **Dispatch:** While concurrency limits allow:
    * **Dequeue:** Asks the `Scheduler` for the next "`Ready`" `block`.
    * **Execute:** Spawns a `Task` on the ThreadPool to execute the `block`.
3. **Block Execution:**
    * **Fetch:** Pulls data from upstream `Warehouses` (triggering JIT Cloning/Moving).
    * **Run:** Executes the `block` logic.
    * **Commit:** Stores results in the `block`'s `Warehouse`.
    * **Notify:** Informs the `Scheduler` of `completion` (which signals downstream `barriers`).
4. **Cycle Management:** When all `blocks` `complete`[^C.11], if sources remain active, resets state and begins the next `shipment` cycle.

## 5. Resource Management

### 5.1. Memory Efficiency (Greedy Completion Pressure)

Since `Warehouses` hold data until *all* consumers have read it, "partial consumption" leads to memory waste. The `Scheduler` picks the `block` that clears the most expensive active buffers.

* **Formula:**

    $$Priority(B) = -\sum_{P \in Predecessors(B)} \frac{WarehouseSize(P)}{RemainingConsumers(P)}$$

    (Negative because PriorityQueue is min-heap)

* **Mechanism (Fan-Out Dilution):**
    * **Shared Parents (Low Priority):** If `Block` A feeds B and C (`Fan-Out / Out-Degree`[^C.28]=2), the pressure to execute B is only $Size(A) / 2$. Executing B doesn't fully release A, so it's "cheaper" to ignore.
    * **Exclusive Parents (High Priority):** Once B executes and produces data for SaveB (`Fan-Out / Out-Degree`=1), the pressure to execute SaveB is $Size(B) / 1$.
    * **Result:** Since $Size(B)/1 > Size(A)/2$, the `scheduler` prioritizes the exclusive child (SaveB) over the sibling (C).

* **Natural DFS:** The formula implicitly favors following a linear chain (where `Fan-Out / Out-Degree`=1) over widening the `graph` (where `Fan-Out / Out-Degree` > 1). This keeps the "working set" of active buffers deep and narrow, rather than shallow and wide.

* **Implementation:** Priority calculated locally at enqueue time (O(`Fan-In / In-Degree`[^C.27])).

### 5.2. Deterministic Disposal

* **Warehouse Cleanup:** Occurs automatically when the last consumer reads the data (Counter reaches 0). Internal reference is nulled (GC eligible).
* **Input Disposal:** Once a consumer `block` finishes execution, the `Engine` calls `workItem.Dispose()` on consumed input `WorkItems`. This disposes the underlying `Image<TPixel>` objects.

    > **In-Place Mutation Handling:** Many processor `blocks` (e.g., Brightness, Contrast, Vignette) mutate the input image in-place using `Image.Mutate()` and return the **same `WorkItem` object** as output. To prevent disposing items that are also stored in the output `Warehouse`, the `Engine` builds a reference-equality `HashSet` of output items and skips disposal for any input item present in that set.
    >
    > ```csharp
    > // Build set of output items to skip (blocks may return mutated inputs)
    > HashSet<IBasicWorkItem>? outputItems = outputs?.Values
    >     .SelectMany(list => list)
    >     .ToHashSet(ReferenceEqualityComparer.Instance);
    >
    > foreach (var item in inputs.Values.SelectMany(list => list))
    > {
    >     if (outputItems == null || !outputItems.Contains(item))
    >         item.Dispose();
    > }
    > ```
    >
    > **Design Rationale:** This approach is forward-compatible with future **filter-type blocks** that drop items based on conditions (e.g., discard images below a quality threshold). Dropped items will not appear in outputs and will be properly disposed.

* **Blocked Blocks:** If a `block` is marked "`Blocked`[^C.9]" (downstream of a failure), the `Engine`:
    1. Atomically decrements upstream `Warehouse` counters for that `block`'s inputs (as if it consumed them). This ensures `Warehouses` are properly cleaned up even when consumers are skipped.
    2. Does NOT execute the `block` (no actual data consumption).
    3. The `Engine` scans for `blocked` downstreams as the consequence of this `block`'s `blocking` and marks them too.

### 5.3. Global Memory Budget (Both Modes)

To prevent OOM, the `Engine` enforces memory limits:

* **Tracking:**
    ```
    TotalMemoryUsage = Σ (WarehouseSize × WorkItem.SizeMP × BytesPerPixel)
    ```
    Updated atomically after each `Warehouse` commit using `Interlocked.Add`.

* **Soft Limit (Default: 50% of available RAM, configurable):**
    - When exceeded, Source `blocks` check `CanProduce()` before execution
    - If over limit: `block` sleeps 100ms, retries (cooperative `backpressure`[^C.30])
    - Existing in-flight `blocks` continue (gradual drain)
    - Priority boost for `blocks` downstream of full `Warehouses` (×2.0)

* **Hard Limit (Default: 75% of available RAM, configurable):**
    - When exceeded, `Engine` has two modes:
        1. **Fail-Fast (Default):** Throw `OutOfMemoryException` with diagnostic data
        2. **Force-Drain (Optional):** Discard oldest `Warehouse` contents, mark consumers as `failed`[^C.10]

* **Memory Estimation:**
    - `BytesPerPixel = 4` for `Rgba32` (default)
    - Configurable per pixel format (`Rgb24 = 3`, `La16 = 2`, etc.)
    - Includes 10% overhead for metadata/internal structures

* **Startup Validation:**
    - If available RAM < 2× largest expected WorkItem, log warning
    - If available RAM < Hard Limit, fail initialization with clear error

## 6. Optimization Strategy: Runtime Adaptation

The `Engine` supports execution modes configured via `ExecutorConfiguration.Mode` (string-based for extensibility):

* **Default Mode**: `"SimpleDfs"` - Depth-first scheduler using greedy completion pressure.

### Mode A: Greedy Completion Pressure (Default, Production-Ready)

Uses pure `Completion Pressure`[^C.26] (see Section 5.1 for formula) with no live profiling or depth calculation. **Recommended for initial implementation and most workloads.**

* **Key Properties:**
    - **Simple:** No depth computation, no arbitrary constants (like ×1000 multipliers)
    - **Greedy:** Always picks the `block` that frees the most memory right now
    - **Natural DFS:** Paths with accumulated `warehouses` become progressively "hungrier"
    - **Overhead:** O(`Fan-In / In-Degree`) per enqueue—negligible for typical `graphs`
    - **Deterministic:** Priority calculated once at enqueue, no runtime updates

* **No Critical Path:** Avoids Bellman-Ford entirely.
* **No Profiling:** Skips cost tracking (Section 6.2/6.3 disabled).
* **Implementation Scope:** Core components only (Sections 1-5, 7-8). Skip Mode B entirely for MVP.

### Mode B: Adaptive (Experimental, High-Complexity Workloads)

⚠️ **DEFER UNTIL POST-MVP.** Extends Mode A with live profiling and `Critical Path`[^C.29] analysis. Use when:
- `Pipelines` have >20 `blocks`
- `Block` costs vary by >5× (e.g., heavy denoise vs. simple crop)
- Willing to accept ~2-5% scheduling overhead for 10-20% throughput gain

**Key Difference from Mode A:** Mode B **extends** Mode A's `Completion Pressure` with cost-aware adjustments.

#### Shipment Boundary Micro-Optimization

**Challenge:** With `shipment`-based execution, the `pipeline` completes a "mini-cycle" after each `shipment` (e.g., every 64 images). Mode B recomputes `critical paths` at these boundaries.

**Options:**

1. **Conservative (Recommended):** Only recompute after full cycles (LoadBlock exhausted).
    - **Pro:** Stable scheduling, avoids thrashing.
    - **Con:** Misses intra-`shipment` optimization opportunities.

2. **Aggressive:** Recompute after every `shipment`.
    - **Pro:** Adapts quickly to changing `block` costs.
    - **Con:** High overhead (157 recomputations for 10,000 images), scheduling instability.

3. **Hybrid (User-Configurable):** Add `ExecutorConfiguration.RecomputeAfterShipment` toggle.
    - **Default:** `false` (conservative)
    - **Advanced Users:** `true` for experimentation

**Implementation Guidance:** Start with Option 1 (conservative). Add Option 3 if profiling shows benefit for specific workloads.

---

### 6.1. Dynamic Priority Adjustment (Mode B Only)

Mode B **extends** Mode A's `Completion Pressure` with runtime cost profiling:

* **Base Priority:** Same as Mode A (Section 5.1):

    $$Priority_{base}(B) = -\sum_{P \in Predecessors(B)} \frac{WarehouseSize(P)}{RemainingConsumers(P)}$$

* **Cost-Weighted Boost (Dynamic):** Applied when dequeueing from Ready Queue:

    $$Priority_{runtime}(B) = Priority_{base}(B) - \alpha \times \hat{Cost}(B) \times AvgInputSize(B)$$

    (Where $Priority_{base}(B)$ is defined in Section 5.1)

    where:
    - $\hat{Cost}(B)$ = profiled cost per megapixel (Section 6.2)
    - $AvgInputSize(B)$ = average input `WorkItem` size in megapixels
    - $\alpha = 0.1$ (tunable, balances memory pressure vs. throughput)
    - **Sign Convention:** Negative because expensive `blocks` should run sooner to prevent downstream starvation. More negative = higher priority (min-heap).

* **Critical Path Multiplier:** If B is on the current `Critical Path` (Section 6.3):

    $$Priority_{runtime}(B) = Priority_{runtime}(B) \times 1.5$$

* **Update Frequency:** Priorities recalculated at dequeue time to reflect current `Warehouse` states and profiled costs (O(`Fan-In / In-Degree` + 1) cost).

* **Fallback Behavior:** If profiling data is insufficient (< 5 samples for `block` type), Mode B falls back to pure Mode A priority (cost term omitted).

> **Rationale:** Mode A's greedy approach provides natural DFS behavior through memory pressure alone. Mode B adds cost-awareness to prioritize expensive `blocks` earlier, preventing late-stage bottlenecks in `pipelines` with heterogeneous `block` costs. The multiplicative `critical path` boost ensures throughput-critical `blocks` aren't starved by memory-hungry but cheap `blocks`.

### 6.2. Incremental Cost Profiling (Mode B Only)

Since execution is non-deterministic, the `Engine` maintains a `Rolling Statistics Window` per `block` type.

* **Metric Collection:** After each `block` execution, record:

    $$Sample_i = \left( T_{exec,i}, PixelCount_i, \frac{T_{exec,i}}{PixelCount_i} \right)$$

    where $PixelCount_i$ is stored in `WorkItem.SizeMP` (megapixels, precomputed: `Width × Height / 1,000,000`)

* **Cost Estimate (Exponential Moving Average):**

    $$\hat{Cost}(B) = \alpha \times \frac{T_{current}}{PixelCount_{current}} + (1-\alpha) \times \hat{Cost}_{previous}(B)$$

    where $\alpha = 0.2$ (emphasizes recent behavior, adapts to thermal throttling).

* **Variance Tracking:** Maintains $\sigma^2$ to detect unstable `blocks` (high variance triggers conservative scheduling).

* **Persistence:** Statistics serialized to `~/.cache/pipeline/profile.db` (SQLite) and restored on next startup.

### 6.3. Critical Path Identification (Live) (Mode B Only)

The `Critical Path` is recomputed **conditionally** to avoid excessive overhead.

> **Note:** `Critical Path`[^C.29] analysis is exclusive to Mode B. Mode A relies solely on `Completion Pressure` without path-based adjustments.

#### Simple Heuristic (Default):

Recompute when ANY of:
1. **Performance Deviation:** Live execution time deviates >20% (configurable) from $\hat{Cost}(B) \times WorkItem.SizeMP$ for `blocks` on the *previous* `Critical Path`.
2. **Sampling Interval:** Every 10 `blocks` (configurable) `complete`.
3. **Time-Based:** Every 5 seconds (configurable) of wall-clock time.

**Algorithm:** Modified Bellman-Ford with negative edge weights:

$$Weight(A \to B) = -\hat{Cost}(A) \times AvgWorkItemSize(A)$$

Longest path from sources to sinks = `Critical Path`. Complexity: O(V + E) per recomputation.

**Priority Boost:** `Blocks` on the `Critical Path` receive multiplicative boost (×1.5) applied to their runtime priority (see Section 6.1).

#### Advanced Strategy (Optional, Experimental):

**Batch Scheduling with Grouped Recomputation:**

* **Mechanism:** Instead of dispatching `blocks` immediately when `ready`, accumulate a "batch" of K `ready blocks` (K=5 default, configurable).
* **Dispatch:** Sort batch by priority, dispatch all K in parallel.
* **Recomputation:** Recalculate `Critical Path` **once** after all K `complete` (amortizes Bellman-Ford cost).
* **Trade-off:**
    - ✅ Reduces scheduling overhead by ~80% (1 recomputation per K `blocks` instead of K).
    - ❌ Introduces latency: `blocks` wait for batch to fill.
    - ❌ Less responsive to sudden cost changes.
* **Best For:** High-throughput batch processing (e.g., 1000+ images).

**Implementation Note:** Enabled via `ExecutionMode.AdaptiveBatched` (distinct from `ExecutionMode.Adaptive`).

### 6.4. Calibration Mode (First-Run Only) (Mode B Only)

On first startup (no cached profiles), the `Engine` runs a `Synthetic Benchmark Suite`:

* **Test Set:** 10 synthetic images per `block` type:
    - 3× 1080p grayscale (1.9 MP)
    - 3× 4K RGB (8.3 MP)
    - 2× 4K RGBA (8.3 MP)
    - 2× 8K grayscale (33.2 MP)

* **Output:** Seeds initial $\hat{Cost}(B)$ estimates with 95% confidence intervals.

* **Fallback:** If calibration fails (e.g., insufficient memory), uses conservative defaults based on `block` category:
    - **Pixel Operations:** 0.5 ms/MP
    - **Convolution Filters:** 5.0 ms/MP
    - **Transforms (FFT, etc.):** 15.0 ms/MP
    - **I/O Operations:** 50.0 ms/MP

### 6.5. Thermal & Load Adaptation (Both Modes)

The `Engine` monitors system conditions and adjusts parallelism:

* **CPU Throttling Detection:** If `block` durations increase >30% across 5 consecutive executions, reduce `MaxDegreeOfParallelism` by 25%.

* **Memory Pressure:** If GC collections exceed 10/second, pause new `block` dispatches until pressure subsides (prevents thrashing).

* **External Load:** Uses `Environment.ProcessorCount` and current CPU usage (via `PerformanceCounter`) to avoid oversubscription.

## 7. Fault Tolerance & Error Handling

### 7.1. Exception Propagation

* **Block Failure:** If a `block` throws an exception:
    1. **Downstream Blocking:** Initiates a `liveness` check to mark affected downstream `blocks` as "`Blocked`"[^C.9].
        * **Premises:**
            * **Persistence:** A `failed` `block` remains `failed`[^C.10] throughout the execution session (persists across `shipment` cycles).
            * **Blocking Nature:** A `failed` `block` is treated as a `blocking` upstream.
            * **Strict Inputs:** Execution requires all input `sockets` to be alive.
            * **Socket Liveness:** An input `socket` is alive if it has at least one incoming connection from a non-`blocking` upstream.
        * **Decision Logic:** When a `block` `fails`, the `Engine` verifies the `liveness` of immediate downstream `blocks`. A downstream `block` is `blocked` if any of its `sockets` lose their last valid connection (i.e., removing the `failed` upstream connection leaves the `socket` with no connections from non-`blocking` upstreams).
        * **Propagation:** Since `blocked blocks` act as `blocking` upstreams, this check propagates recursively (BFS) to ensure all transitively affected `blocks` are identified.
    2. **Warehouse Counter Cleanup:** For each `blocked` `block`, atomically decrements its upstream `Warehouse` counters (as if it consumed the data). This ensures `Warehouses` are released even when consumers don't run.
    3. Collects the exception into an `AggregateException`.
    4. Allows independent `pipeline` branches to continue (partial failure tolerance).
* **Fatal Errors:** Out-of-memory or stack overflow abort the entire `pipeline` immediately.

### 7.2. Cancellation Support

* **Cooperative Cancellation:** `Engine` accepts `CancellationToken`. `Blocks` poll the token before heavy operations.
* **Cleanup:** On cancellation, the `Engine` disposes all `Warehouses` and marks the `pipeline` as "Aborted".

> **Note:** `Block` **MUST** implement support for cancellation token to accept cancellation request from `Engine`.

## 8. Implementation Notes

### 8.1. Data Structures

* **Ready Queue:** `System.Collections.Generic.PriorityQueue<IBlock, float>` (guarded by lock).
* **Warehouse Storage:** `ImmutableList<WorkItem>` (guarded by lock for write/clear, lock-free for read).
* **Barrier Counter:** `int` with `Interlocked` operations (lock-free).
* **Lazy Components:** `ConcurrentDictionary<IBlock, Lazy<T>>` for on-demand initialization.

### 8.2. Threading Model

* **Execution:**
    - **Default:** `Task.Run` (runs on ThreadPool).
    - **Long Running:** Not yet implemented (Mode B).
* **Coordination:** Single async loop in `GraphExecutor`.

### 8.3. Concurrency & Synchronization

The system employs a **Hybrid Concurrency Model** combining atomic operations for high-frequency counters and fine-grained locks for structural state:

* **Atomic Operations (`Interlocked`):**
    - Used for `Dependency Barriers` (signaling `readiness`).
    - Used for `Warehouse Consumer Counts` (tracking remaining consumers).
    - Ensures minimal overhead for the most frequent operations (signaling and checking availability).

* **Fine-Grained Locks (`System.Threading.Lock`):**
    - **Scheduler Queue:** Protects the PriorityQueue. Since enqueue/dequeue frequency is lower than `block` execution time, contention is low.
    - **Warehouse Inventory:** Protects the import (commit) and the final cleanup (when count reaches zero).
    - **Active Sources:** Protects the list of active `shipment` sources.

* **Rationale:**
    - Pure lock-free structures (like `ConcurrentPriorityQueue`) add significant complexity and are often slower than uncontended locks in .NET.
    - The hybrid approach balances simplicity and performance, using atomics where contention is highest.

### 8.4. WorkItem Structure

```csharp
public sealed class WorkItem : IDisposable
{
    public Image Image { get; }       // SixLabors.ImageSharp
    public float SizeMP { get; }      // Width × Height / 1,000,000
    public IImmutableDictionary<string, object> Metadata { get; }

    public WorkItem Clone() => new WorkItem(
        Image.Clone(x => {}),         // Deep copy of pixel data
        SizeMP,
        Metadata                      // Shallow copy (immutable)
    );

    public void Dispose() => Image.Dispose(); // Release unmanaged pixel buffer
}
```

## 9. References

1. **POSA:** Buschmann, F., et al. (1996). *Pattern-Oriented Software Architecture Volume 1*. (Pipes and Filters).
2. **EIP:** Hohpe, G., & Woolf, B. (2003). *Enterprise Integration Patterns*. (Message Store, Content-Based Router).
3. **GoF:** Gamma, E., et al. (1994). *Design Patterns*. (Observer, Prototype).
4. **Memory Management:** Jones, R., et al. (2011). *The Garbage Collection Handbook*. (Reference Counting).
5. **Concurrent Data Structures:** Herlihy, M., & Shavit, N. (2012). *The Art of Multiprocessor Programming*. (Lock-free algorithms).
6. **Graph Algorithms:** Cormen, T. H., et al. (2009). *Introduction to Algorithms, 3rd Ed*. (Topological sort, critical path).
7. **Adaptive Systems:** Hellerstein, J. L., et al. (2004). *Feedback Control of Computing Systems*. (Runtime adaptation, PID controllers).

---

## A. Appendix: Future Enhancements & Known Limitations

### A.1. Scalability: Priority Queue Contention

**Current Bottleneck:**
Under extreme parallelism (>100 concurrent threads), the single `ConcurrentPriorityQueue` can become a contention point even with lock-free operations. The head node experiences high CAS retry rates during concurrent dequeues.

**Proposed Solution: Two-Tier Scheduling**

Partition the `ready` queue into two tiers:

* **Tier 1 (Fast Path):** FIFO queue for `blocks` with uniform priority (Mode A) or low memory pressure. Uses work-stealing deques per thread to eliminate contention.
* **Tier 2 (Slow Path):** Priority queue for memory-pressure-sensitive `blocks` (high `Completion Pressure` score). Only accessed when Tier 1 is empty or memory exceeds thresholds.

**Trade-offs:**
- ✅ Reduces contention by 80-90% (measured in Go's runtime `scheduler`)
- ✅ Maintains priority semantics where needed (memory-critical `blocks`)
- ❌ Adds significant complexity: per-thread queues, work-stealing protocol, queue selection heuristics
- ❌ Requires ~50% more code in the `scheduler`

**Recommendation:** Implement only if profiling shows >10% time spent in queue operations under production workloads. Current design is sufficient for ≤100 threads.

---

### A.2. Memory Management: Predictive Backpressure

**Current Limitation:**
Section 6.5's GC-based throttling is **reactive** (waits for 10 GC/sec before pausing). The system can accumulate excessive in-flight memory before throttling triggers, leading to:
- Sudden pauses (all dispatches halt)
- Risk of OOM on memory-constrained systems
- No global cap on total `Warehouse` memory

**Critical Gap Example:**
```
10 Source `blocks` × 10 images/sec × 8K (32MB/image) = 3.2 GB/sec production rate
If downstream is 2× slower → 6.4 GB accumulation in 2 seconds
System with 8GB RAM → OOM before GC throttling activates
```

**Proposed Solution: Global Memory Budget with Soft/Hard Limits** (Section 5.3)

**Implementation Priority:** **CRITICAL**. This is a production blocker—must be added before release.

---

### A.3. Clarification: Priority Staleness

**Issue:** The term "lazy priority recalculation" could be misinterpreted as "priorities can be stale at dequeue time."

**Actual Behavior (Both Modes):**

Priorities are computed **on-demand during dequeue**, not cached:

* **Mode A:** Each `Dequeue()` calculates `Completion Pressure` from current `Warehouse` states.
* **Mode B:** Each `Dequeue()` calculates `Completion Pressure` + Cost-Weighted Boost + `Critical Path` Multiplier.

**Dequeue Process:**
1. Peek at top K candidates (K=3 default)
2. Recalculate `Priority(B)` for each using current `Warehouse` states (and profiled costs in Mode B)
3. Select highest priority (most negative value)
4. Return that `block`

**Cost:**
- Mode A: O(K × `Fan-In / In-Degree`) = ~6-9 operations per dequeue
- Mode B: O(K × (`Fan-In / In-Degree` + 1)) = ~9-12 operations per dequeue (adds cost lookup)

**No architectural change needed**—this is a documentation clarification.

---

### A.4. Advanced Research: Adaptive Work Batching

**Concept:** Instead of recalculating priorities per-`block`, accumulate batches of K `ready blocks` before scheduling.

**Mechanism:**
1. Wait until K `blocks` are `ready` (K=5 default)
2. Sort batch by priority once
3. Dispatch all K in parallel
4. Recalculate `Critical Path` once after all K `complete`

**Benefits:**
- Amortizes Bellman-Ford cost (O(V+E) every K `blocks` instead of every `block`)
- Reduces priority calculation overhead by ~80%

**Drawbacks:**
- Adds latency: `blocks` wait for batch to fill
- Less responsive to dynamic changes (e.g., sudden memory pressure)
- Requires careful tuning of K (too large = latency, too small = no benefit)

**Status:** Mentioned in Section 6.3 as `ExecutionMode.AdaptiveBatched`. Experimental—do not enable for production until validated with real workloads.

---

### A.5. Known Limitations

1. **No GPU Memory Tracking:** Current memory budget only tracks CPU RAM. GPU-accelerated `blocks` (if implemented) would need separate tracking.
2. **No Network I/O Backpressure:** If `blocks` fetch from network sources, the memory budget doesn't account for in-flight network buffers.
3. **Barrier Allocation:** Even with lazy initialization, `Barriers` are allocated per-`block` (not per-execution). A `graph` with 1000 `blocks` uses ~8KB for `Barriers` even if only 10 run concurrently.
4. **Priority Queue Scalability:** Single queue design limits scalability beyond ~100 threads (see A.1).
5. **No Priority Inheritance:** If a high-priority `block` depends on a low-priority `block`, the system doesn't propagate priority (can cause priority inversion under heavy load).

---

### A.6. Observability and Telemetry

**Current Limitation:** Debugging parallel `graph` execution via logs is difficult due to interleaved execution traces.

**Proposed Enhancement:** Integrate OpenTelemetry .NET instrumentation to produce distributed traces:
    - **Spans:** One per `block` execution, tagged with `BlockID`, `ThreadID`, and `ExecutionMode`
    - **Visualization:** Export traces to Jaeger/Zipkin to visualize `Critical Paths` and "waterfall" execution diagrams

---

### A.7. Static Analysis: Strict Paths & Threatened Blocks

The static analyzer performs offline `graph` analysis to identify structural sensitivity patterns before execution.

**Definitions:**

* `Strict Block`[^C.15]: A `block` with `Fan-In / In-Degree`[^C.27] = 1 (exactly one incoming connection from a single upstream `block`). The `block` has no "backup" connection—if its sole upstream `fails`, the `strict block` becomes `blocked`.

* `Strict Path`[^C.16]: The `transitive closure` of upstream dependencies originating from a `strict block`. Formally, for `strict block` `B`, the `strict path` is the set `{ P | P →* B }`, where `→*` denotes the reflexive transitive closure of the "depends on" relation.

* `Threatened`[^C.17] `Block`: A `block` **not** on any `strict path` that may become `blocked` if one of its upstream dependencies `fails`. A `block` is `threatened` if there exists at least one upstream `block` `U` such that:
    1. `U` is not on a `strict path` leading to the `block`
    2. Removing `U` from the `graph` leaves the `block` with no viable input path from any source

**Example:**

```
Source → [Resize] → [Enhance] → [Save]
                └──→ [Crop] ────────┘
```

* **Strict Block:** `Enhance` (`Fan-In / In-Degree` = 1, depends solely on `Resize`)
* **Strict Path:** `{ Source, Resize, Enhance }`
* **Threatened Block:** `Save`—if `Crop` `fails`, `Save` still has input via `Enhance`. If `Resize` `fails`, both `Enhance` and `Save` become `blocked`.

**Analysis Output:**

The analyzer emits a report containing:
1. **Strict Path Enumeration:** List of all `strict paths`, indexed by `strict block ID`
2. **Threatened Block Registry:** `Blocks` vulnerable to specific upstream `failures`, with dependency mapping
3. **Criticality Score:** `ThreatenedCount / TotalBlockCount`—higher values indicate reduced fault tolerance

**Interpretation:**

`Strict paths` and `threatened blocks` are `structural properties`, not defects. They represent:
* **Sensitivity:** Single points of failure in the computation topology
* **Trade-off:** Linear chains (all `strict`) minimize memory overhead (Section 5.1) at the cost of fault isolation
* **Design Guidance:** For mission-critical `pipelines`, introduce redundant branches to eliminate `strict paths`

**Integration with Execution:**

When a `block` `fails` during runtime (Section 7.1), the `Engine` uses precomputed `strict paths` to efficiently determine the transitive closure of `blocked` downstream `blocks`—O(1) lookup per `strict block` instead of BFS traversal.

## B. Appendix: Diagrams

### B.1. High-level diagram

```mermaid
flowchart TB
    subgraph Control_Plane [Control Plane: Orchestration & Signaling]
        direction TB
        Engine[("<b>The Engine</b><br/>(Process Manager)")]
        ReadyQ[("<b>Ready Queue</b><br/>(Priority Sorted)")]
        Barrier[("<b>Dependency Barrier</b><br/>(Atomic Counter)")]
    end

    subgraph Data_Plane [Data Plane: Storage & Flow]
        direction TB
        Producer[("<b>Producer Block</b><br/>(Filter)")]
        Warehouse[("<b>The Warehouse</b><br/>(Output Buffer & Inventory)")]
        Consumer[("<b>Consumer Block</b><br/>(Filter)")]
    end

    %% Execution Flow
    Producer -- "1. Executes & Commits Result" --> Warehouse
    Producer -- "2. Notify Completion" --> Engine

    %% Signaling Flow
    Engine -- "3. Decrement Counter" --> Barrier
    Barrier -- "4. Counter == 0? Signal Ready" --> ReadyQ
    ReadyQ -- "5. Dispatch (DFS Priority)" --> Engine
    Engine -- "6. Schedule on ThreadPool" --> Consumer

    %% Data Pull Flow
    Consumer -- "7. Pull Data (JIT Clone)" --> Warehouse
    Warehouse -- "8. Return WorkItem" --> Consumer
```

### B.2. Detailed Diagram

```mermaid
flowchart LR
    subgraph Orchestrator ["The Engine (Orchestrator)"]
        direction TB
        Lifecycle[("<b>Lifecycle Manager</b><br/>Graph Validation & Instantiation")]
        Scheduler[("<b>Scheduler</b><br/>DFS Optimization Strategy")]
        ErrorHandler[("<b>Error Propagator</b><br/>Blocking & Cancellation")]
        Context[("<b>ExecutionContext</b><br/>Execution session internal states")]

        Lifecycle --> Scheduler
    end

    subgraph Execution ["Worker Environment"]
        Thread[".NET ThreadPool Worker"]
        WorkItem["<b>WorkItem</b><br/>Image&lt;TPixel&gt; + Metadata"]
    end

    subgraph Storage ["The Warehouse (Producer-Centric)"]
        direction TB
        Buffer[("<b>Immutable Buffer</b><br/>Dict&lt;Socket, List&lt;WorkItem&gt;&gt;")]
        ConsCounter[("<b>Consumer Counter</b><br/>Atomic Int32 (Fan-Out / Out-Degree)")]
        JITLogic{"<b>JIT Logic</b><br/>(Interlocked.Decrement)"}

        Buffer
        JITLogic --> ConsCounter
        ConsCounter --> Clone["<b>Defensive Clone</b><br/>(Deep Copy)"]

    end

    subgraph Synchronization ["The Barrier (Consumer-Centric)"]
        direction TB
        DepCounter[("<b>Dependency Counter</b><br/>Atomic Int32 (Fan-In / In-Degree)")]
        EnqFlag[("<b>Enqueue Flag</b><br/>Atomic CAS (1 bit)")]

        DepCounter -- "Reach 0" --> EnqFlag
        EnqFlag -- "CompareExchange(0,1)" --> Signal["Signal Ready"]
    end

    %% Interactions
    Signal --> Scheduler
    Scheduler -- "Dequeue(Priority)" --> Thread
    Thread -- "Execute()" --> WorkItem
    Thread -- "Fetch Input" --> JITLogic
    Thread -- "Commit Output" --> Buffer

    %% Priority Logic Note
    Scheduler -.- PriorityNote["<b>Priority Formula:</b><br/>Sum(WarehouseSize / RemainingConsumers)"]
```

### B.3. Execution Diagram

```mermaid
%% { init: {"flowchart": {"defaultRenderer": "elk"}} }%%
sequenceDiagram
    autonumber
    participant Engine
    participant ThreadPool

    box Upstream
    participant BlockA as Block A (Source)
    participant WhA as Warehouse A
    end

    box Parallel Chain B
    participant BlockB as Block B
    participant WhB as Warehouse B
    end

    box Parallel Chain C
    participant BlockC as Block C
    participant WhC as Warehouse C
    end

    box Downstream
    participant BarrD as Barrier D
    participant BlockD as Block D (Sink)
    end

    note over Engine: PHASE 1: INITIALIZATION
    Engine->>ThreadPool: Schedule Source (A)
    activate BlockA
    BlockA->>WhA: Commit Result
    deactivate BlockA
    BlockA->>Engine: Notify Complete

    Engine->>Engine: Enqueue B & C

    note over Engine: PHASE 2: PARALLEL EXECUTION

    par Chain B
        Engine->>ThreadPool: Dispatch B
        activate BlockB

        note right of BlockB: 1. Gather
        BlockB->>WhA: Fetch Input (Clone)

        note right of BlockB: 2. Execute
        BlockB->>BlockB: Process

        note right of BlockB: 3. Push
        BlockB->>WhB: Commit Result
        deactivate BlockB

        note left of Engine: 4. Signal
        BlockB->>Engine: Notify Complete
        Engine->>BarrD: Signal (Decr 2->1)
    and Chain C
        Engine->>ThreadPool: Dispatch C
        activate BlockC

        note right of BlockC: 1. Gather
        BlockC->>WhA: Fetch Input (Clone)
        note right of WhA: Ref Cleared

        note right of BlockC: 2. Execute
        BlockC->>BlockC: Process

        note right of BlockC: 3. Push
        BlockC->>WhC: Commit Result
        deactivate BlockC

        note right of Engine: 4. Signal
        BlockC->>Engine: Notify Complete
        Engine->>BarrD: Signal (Decr 1->0)
    end

    BarrD-->>Engine: Barrier Open (Ready)

    note over Engine: PHASE 3: MERGE
    Engine->>ThreadPool: Dispatch D
    activate BlockD
    BlockD->>WhB: Fetch Input
    BlockD->>WhC: Fetch Input
    BlockD->>BlockD: Execute
    deactivate BlockD
```

### B.4. Shipment Lifecycle Diagram

```mermaid
flowchart TD
    subgraph Cycle_Management [Shipment Cycle Loop]
        Start((Start)) --> InitSource["Init IShipmentSource<br/>(Cursor = 0)"]
        InitSource --> ExecSource[Execute Source Block]

        ExecSource --> CheckYield{Output Count >=<br/>MaxShipmentSize?}

        CheckYield -- Yes (More Data) --> MarkActive[Mark Source 'Active']
        MarkActive --> EnqueueDownstream[Signal Downstream Barriers]

        CheckYield -- No (Exhausted) --> MarkComp[Mark Source 'Completed']
        MarkComp --> EnqueueDownstream

        EnqueueDownstream --> Drain[<b>Drain Pipeline</b><br/>Execute all reachable blocks]

        Drain --> AllDone{All Blocks<br/>Finished?}
        AllDone -- No --> Drain
        AllDone -- Yes --> CheckSources{Any Source<br/>Active?}

        CheckSources -- Yes --> ResetGraph[Reset Barriers &<br/>Re-enqueue Sources]
        ResetGraph --> ExecSource

        CheckSources -- No --> Finish((End))
    end
```

---

## C. Appendix: Lexicon

This section standardizes terminology across the Execution Engine specification, adopting federated consensus concepts to describe graph liveness, safety, and state transitions.

[^C.1]: Stateless processing unit implementing `IBlock`. Transforms input `WorkItem`s into output `WorkItem`s.
[^C.2]: Named input or output port on a block. Defines the typed interface for data flow.
[^C.3]: Directed connection between sockets. Establishes producer-consumer relationship.
[^C.4]: Logical subset of blocks for analysis (e.g., "strict path subgraph"). Not a formal graph cut.
[^C.5]: Complete DAG of blocks and links. Validated for acyclicity before execution.
[^C.6]: Synonym for the execution graph during a processing session.
[^C.7]: Block with all dependency barriers satisfied. Eligible for dispatch.
[^C.8]: Dispatched block awaiting completion. In-flight execution.
[^C.9]: Block unable to execute due to upstream failure. Transitive—propagates downstream.
[^C.10]: Block that threw an exception. Marks downstream as blocked.
[^C.11]: Block that finished successfully. Output committed to warehouse.
[^C.12]: Property that system continues making progress. Blocks complete; watchdog detects violations.
[^C.13]: Property that no `WorkItem` is lost or corrupted. Enforced via cloning and cleanup.
[^C.14]: Completed block output is immutable and never rolled back.
[^C.15]: Block with In-Degree = 1. Single point of failure if upstream fails.
[^C.16]: Transitive closure of upstream dependencies from a strict block.
[^C.17]: Block that may become blocked if specific upstream fails. Precomputed by analyzer.
[^C.18]: Set of blocks that must complete to unblock a strict downstream.
[^C.19]: Set of upstream blocks whose failure prevents downstream execution.
[^C.20]: Producer-centric output buffer holding `WorkItem`s until all consumers consume.
[^C.21]: Consumer-centric synchronization primitive with atomic dependency counter.
[^C.22]: Central orchestrator managing lifecycle, scheduling, error handling, resources.
[^C.23]: Policy component determining dispatch order via Completion Pressure or Adaptive strategies.
[^C.24]: Batch of `WorkItem`s from a source in one cycle. Enables memory-controlled processing.
[^C.25]: Immutable wrapper of `Image<TPixel>` with metadata. Disposable.
[^C.26]: Scheduling priority based on warehouse memory. $-\sum (Size / RemainingConsumers)$.
[^C.27]: Number of incoming links. Determines barrier counter initialization.
[^C.28]: Number of outgoing links. Determines warehouse counter initialization.
[^C.29]: Longest path from source to sink. Used in Mode B for priority boosting.
[^C.30]: Flow control limiting production when downstream exhausted. Via soft/hard memory limits.
[^C.31]: Liveness violation—no blocks ready but execution incomplete. Detected by watchdog.